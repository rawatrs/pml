{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [5, 6], [6, 7], [8, 9], [9, 10], [11, 12]])\n",
    "Y = np.array([0, 0, 0, 1, 1, 1, 2, 2, 2])\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)\n",
    "print(X_train)\n",
    "print(Y_train)\n",
    "print(X_test)\n",
    "print(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kfold train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=3, random_state=None, shuffle=False)\n",
      "train indices: [3 4 5 6 7 8] test indices: [0 1 2]\n",
      "train indices: [0 1 2 6 7 8] test indices: [3 4 5]\n",
      "train indices: [0 1 2 3 4 5] test indices: [6 7 8]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [5, 6], [6, 7], [8, 9], [9, 10], [11, 12]])\n",
    "Y = np.array([0, 0, 0, 1, 1, 1, 2, 2, 2])\n",
    "kf = KFold(n_splits=3)\n",
    "\n",
    "print(kf)  \n",
    "for train_index, test_index in kf.split(X, Y):\n",
    "   print(\"train indices:\", train_index, \"test indices:\", test_index)\n",
    "   X_train, X_test = X[train_index], X[test_index]\n",
    "   Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "#    print(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified KFold split: Classes are distributed across the splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StratifiedKFold(n_splits=3, random_state=None, shuffle=False)\n",
      "train indices: [1 2 4 5 7 8] test indices: [0 3 6]\n",
      "train indices: [0 2 3 5 6 8] test indices: [1 4 7]\n",
      "train indices: [0 1 3 4 6 7] test indices: [2 5 8]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [5, 6], [6, 7], [8, 9], [9, 10], [11, 12]])\n",
    "Y = np.array([0, 0, 0, 1, 1, 1, 2, 2, 2])\n",
    "skf = StratifiedKFold(n_splits=3)\n",
    "\n",
    "print(skf)  \n",
    "for train_index, test_index in skf.split(X, Y):\n",
    "   print(\"train indices:\", train_index, \"test indices:\", test_index)\n",
    "   X_train, X_test = X[train_index], X[test_index]\n",
    "   Y_train, Y_test = Y[train_index], Y[test_index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Varying neural networks hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.datasets import mnist\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "\n",
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(40, activation='relu', input_shape=(28*28,)))\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dense(10, activation='sigmoid'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                optimizer=SGD(),\n",
    "                metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape(60000, 28*28)\n",
    "X_test = X_test.reshape(10000, 28*28)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# normalize\n",
    "X_train = X_train/255\n",
    "X_test = X_test/255\n",
    "\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# Encode Y as binary class vector\n",
    "Y_train = np_utils.to_categorical(Y_train, 10)\n",
    "Y_test = np_utils.to_categorical(Y_test, 10)\n",
    "\n",
    "X = X_train\n",
    "Y = Y_train\n",
    "\n",
    "#Wrapper to make keras model work with scikitlearn\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "batch_size = [32, 50, 100]\n",
    "epochs = [5, 7, 10]\n",
    "\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv = 3)\n",
    "grid_result = grid.fit(X, Y)\n",
    "\n",
    "# # summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Varying neural network's layers and neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.datasets import mnist\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.utils import np_utils\n",
    "\n",
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "\n",
    "def create_model(model_conf):\n",
    "    model = Sequential()\n",
    "\n",
    "    #add hidden layers \n",
    "    #print('building nn with conf' + str(model_conf))\n",
    "    hidden_layers = model_conf['hidden_layers']\n",
    "    n_h1 = hidden_layers[0]['neurons'] \n",
    "    n_h1_activation = hidden_layers[0]['activation']\n",
    "    model.add(Dense(n_h1, activation=n_h1_activation, input_shape=(28*28,)))\n",
    "    for layer in hidden_layers[1:]:\n",
    "        neurons = layer['neurons']\n",
    "        activation = layer['activation'] \n",
    "        model.add(Dense(neurons, activation=activation))\n",
    "    \n",
    "    #add output_layer     \n",
    "    model.add(Dense(10, activation='sigmoid'))\n",
    "    optimizer = model_conf['optimizer']\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape(60000, 28*28)\n",
    "X_test = X_test.reshape(10000, 28*28)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# normalize\n",
    "X_train = X_train/255\n",
    "X_test = X_test/255\n",
    "\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# Encode Y as binary class vector\n",
    "Y_train = np_utils.to_categorical(Y_train, 10)\n",
    "Y_test = np_utils.to_categorical(Y_test, 10)\n",
    "\n",
    "X = X_train\n",
    "Y = Y_train\n",
    "\n",
    "#Wrapper to make keras model work with scikitlearn\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "# define the grid search parameters\n",
    "\n",
    "batch_size = [32]\n",
    "epochs = [5]\n",
    "model_conf = [\n",
    "    { \"hidden_layers\": [{\"neurons\": 40, \"activation\": \"relu\"},\n",
    "                         {\"neurons\": 20, \"activation\": \"relu\"}\n",
    "                        ],\n",
    "       \"optimizer\": SGD(lr=0.005)\n",
    "    },\n",
    "    { \"hidden_layers\": [{\"neurons\": 30, \"activation\": \"relu\"},\n",
    "                         {\"neurons\": 15, \"activation\": \"relu\"},\n",
    "                        ],\n",
    "       \"optimizer\": SGD(lr=0.005)\n",
    "    },\n",
    "    { \"hidden_layers\": [{\"neurons\": 60, \"activation\": \"relu\"},\n",
    "                         {\"neurons\": 50, \"activation\": \"relu\"},\n",
    "                        {\"neurons\": 40, \"activation\": \"relu\"},\n",
    "                        {\"neurons\": 30, \"activation\": \"relu\"},\n",
    "                        {\"neurons\": 20, \"activation\": \"relu\"}\n",
    "                       ],\n",
    "       \"optimizer\": SGD()\n",
    "    },\n",
    "    { \"hidden_layers\": [{\"neurons\": 40, \"activation\": \"relu\"},\n",
    "                         {\"neurons\": 20, \"activation\": \"relu\"}\n",
    "                        ],\n",
    "       \"optimizer\": Adam()\n",
    "    }\n",
    "\n",
    "\n",
    "]\n",
    "\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs, model_conf=model_conf)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1, cv = 2, refit=False)\n",
    "grid_result = grid.fit(X, Y)\n",
    "\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[1000, 2, 0.0001], [2000, 4, 0.0003], [3000, 2, 0.0005]])\n",
    "scaled_X = preprocessing.scale(X)\n",
    "print(scaled_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean Squared Error for Regression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "Y = [100, 200, 300, 400]\n",
    "Y_hat = [102, 198, 300, 405]\n",
    "\n",
    "mse = mean_squared_error(Y, Y_hat)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy for Classification Problems\n",
    "from sklearn.metrics import accuracy_score\n",
    "Y = [1, 1, 0, 0, 1]\n",
    "Y_hat = [1, 1, 0, 0, 0 ]\n",
    "accuracy = accuracy_score(Y, Y_hat)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation metrics for skewed classes\n",
    "from sklearn.metrics import accuracy_score\n",
    "# 0 is normal class 1 is nearly outlier class\n",
    "Y = [ 0 for i in range(1000)] + [1 for i in range(10)]\n",
    "\n",
    "def dumb_classifier_predict(N):\n",
    "    #returns class 0 for everything except some random ones\n",
    "    res = [0 for i in range(N)]\n",
    "    res[0] = res[5] = res[1002] = 1\n",
    "    return res\n",
    "\n",
    "Y_hat = dumb_classifier_predict(1010)\n",
    "\n",
    "accuracy = accuracy_score(Y, Y_hat)\n",
    "print('accuracy=', accuracy)\n",
    "#even dumb classifiers can have high accuracy in skewed class scenarios\n",
    "#Use precision and recall\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "precision = precision_score(Y, Y_hat)\n",
    "print('precision =', precision)\n",
    "recall = recall_score(Y, Y_hat)\n",
    "print('recall =', recall)\n",
    "\n",
    "#For single valued metric on skewed classes use F1 score which combines precision and recall\n",
    "f1 = f1_score(Y, Y_hat)\n",
    "print('f1 score =',f1)\n",
    "\n",
    "#Confusion metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(Y, Y_hat))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion matrix for multiclass classification\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "\n",
    "Y = [ 0 for i in range(1000)] + [1 for i in range(1000)] + [2 for i in range(1000)] + [3 for i in range(1000)] + [4 for i in range(1000)]\n",
    "Y_hat = [ random.randint(0,4) for i in range(5000)]\n",
    "\n",
    "#Confusion matrix for multiclass classification\n",
    "print(confusion_matrix(Y, Y_hat))\n",
    "\n",
    "#Precision, Recall, F1 score for each class\n",
    "print(precision_recall_fscore_support(Y, Y_hat))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
