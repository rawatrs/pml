{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "def square(x):\n",
    "#TBD\n",
    "    return x*x\n",
    "\n",
    "def times2(x):\n",
    "#TBD\n",
    "    return x*2\n",
    "\n",
    "def average(x1, x2, x3, x4, x5):\n",
    "#TBD\n",
    "    return (x1+x2+x3+x4+x5)/5\n",
    "\n",
    "print(times2(10) == 20)\n",
    "print(square(6) == 36)\n",
    "print(average(1,2,3,4,5) == 3.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reverse engineering functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "'''\n",
    "+--------+--+--------+--+--------+--+--------------+\n",
    "| f1     |  | f2     |  | f3     |  | f4           |\n",
    "+--------+  +--------+  +--------+  +--------------+\n",
    "| x | y  |  | x | y  |  | x  | y |  | x1 | x2 | y  |\n",
    "+---+----+  +---+----+  +----+---+  +----+----+----+\n",
    "| 3 | 6  |  | 3 | 6  |  | 1  | 0 |  | 2  | 3  | 4  |\n",
    "+---+----+  +---+----+  +----+---+  +----+----+----+\n",
    "| 7 | 14 |  | 7 | 28 |  | 7  | 1 |  | 8  | 5  | 9  |\n",
    "+---+----+  +---+----+  +----+---+  +----+----+----+\n",
    "| 8 | 16 |  | 4 | 10 |  | 3  | 1 |  | 6  | 4  | 7  |\n",
    "+---+----+  +---+----+  +----+---+  +----+----+----+\n",
    "| 1 | 2  |  | 1 | 1  |  | 2  | 1 |  | 10 | 2  | 7  |\n",
    "+---+----+  +---+----+  +----+---+  +----+----+----+\n",
    "| 5 | 10 |  | 9 | 45 |  | 9  | 0 |  | 4  | 2  | 4  |\n",
    "+---+----+  +---+----+  +----+---+  +----+----+----+\n",
    "| 9 | 18 |  | 6 | 21 |  | 12 | 0 |  | 16 | 7  | 15 |\n",
    "+---+----+--+---+----+--+----+---+--+----+----+----+\n",
    "'''\n",
    "def f1(x):\n",
    "    return 2*x\n",
    "\n",
    "def f2(x):\n",
    "    return x*(x+1)/2\n",
    "\n",
    "def f4(x1, x2):\n",
    "    return x1/2 + x2\n",
    "\n",
    "print(f1(11) == 22)\n",
    "print(f2(5) == 15.0)\n",
    "print(f4(10, 7) == 12.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Software 2.0?\n",
    "[Andrej Karpathy](https://medium.com/@karpathy/software-2-0-a64152b37c35)\n",
    "\n",
    " - 1.0\n",
    "     - Function\n",
    "     - Specifications, Code, small {X,Y} which is used to test specs\n",
    "     - Code\n",
    "     - Concrete Answers\n",
    " - 2.0 \n",
    "     - Model\n",
    "     - Large {X,Y} which is used to learn/test a model\n",
    "     - Weights\n",
    "     - Soft Answers\n",
    "\n",
    "### Machine Learning and Software 2.0\n",
    "\n",
    "### Types of ML\n",
    "    1. Supervised Learning\n",
    "        - Regression\n",
    "        - Classification\n",
    "    2. Unsupervised Learning\n",
    "    3. Reinforcement Learning\n",
    "\n",
    "### When to apply ML?\n",
    "    1. Pattern exists\n",
    "    2. No mathematical formula\n",
    "    3. Data available\n",
    "\n",
    "\n",
    "### Features, Signals, Attributes, *x*\n",
    " - Text as features\n",
    " - Videos, Pictures as features\n",
    " - Time as features\n",
    " \n",
    "### Label, Targets, *y*\n",
    "\n",
    "### *x* and *y* \n",
    "\n",
    "### *X* and *Y*\n",
    "\n",
    "### *n* and *N*\n",
    "\n",
    "### Dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This Class:\n",
    "\n",
    "### What Level?\n",
    " - Input: Numbers.\n",
    " - Models: Some canned, Some from scratch\n",
    " - Evaluation: Some canned, Some from scratch\n",
    " \n",
    "### Deep Dives\n",
    " - ML Evaluation Metrics\n",
    " - Linear Regression\n",
    " - Neural Network (Back Propogation Algorithm)\n",
    " - Collaborative Filtering\n",
    " - Graph Centrality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression (Predicting House Prices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Description ==> Boston House Prices dataset\n",
      "===========================\n",
      "\n",
      "Notes\n",
      "------\n",
      "Data Set Characteristics:  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive\n",
      "    \n",
      "    :Median Value (attribute 14) is usually the target\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "http://archive.ics.uci.edu/ml/datasets/Housing\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      "**References**\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)\n",
      "\n",
      "==========\n",
      "\n",
      "features ==> ['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
      " 'B' 'LSTAT']\n",
      "[ 1.20125417  0.01484179  0.02355189  0.60206504 -8.82764179  9.1306223\n",
      " -0.04735918 -1.01328587  0.16786579 -0.01456734 -0.64183404  0.01677883\n",
      " -0.10976398]\n",
      "Predicted values: [19.73321195 25.54082888 21.18204475 28.83841101 26.94137539 20.0800139 ]\n",
      "Actual values: [16.8 22.4 20.6 23.9 22.  11.9]\n",
      "4.119314314513935\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Load Boston, house price dataset\n",
    "'''\n",
    "from sklearn import datasets\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "boston = datasets.load_boston()\n",
    "\n",
    "# print(f'Boston dataset info: {boston.keys()}')\n",
    "# print(f'Dataset Description ==> {boston[\"DESCR\"]}\\n==========\\n')\n",
    "# print(f'X ==> boston[\"data\"]')\n",
    "# print(f'Y ==> boston[\"target\"]')\n",
    "\n",
    "print(f'features ==> {boston[\"feature_names\"]}')\n",
    "# print(f'n ==> {len(boston[\"feature_names\"])}')\n",
    "# print(f'N ==> {len(boston[\"data\"])}')\n",
    "# print(f'Shape of X ==> {boston[\"data\"].shape}')\n",
    "# print(f'Shape of Y ==> {boston[\"target\"].shape}')\n",
    "# print(f\"First data point: x, X[0]: {boston.data[0]}\")\n",
    "# print(f\"First label: y, Y[0]: {boston.target[0]}\")\n",
    "\n",
    "\n",
    "N = 300\n",
    "X_train = boston.data[:N]\n",
    "Y_train = boston.target[:N]\n",
    "X_test = boston.data[500:]\n",
    "Y_test = boston.target[500:]\n",
    "\n",
    "model = linear_model.LinearRegression()\n",
    "\n",
    "# TODO 1: Fit the model to training data\n",
    "model.fit(X_train,Y_train)\n",
    "# TODO 2: Use fitted model to predict the house prices of houses in test data \n",
    "print(model.coef_)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "print(f\"Predicted values: {predictions}\")\n",
    "print(f\"Actual values: {Y_test}\")\n",
    "print(metrics.mean_absolute_error(Y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification (Detecting Spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ['bar', 'foo', 'hello', 'lottery', 'prize', 'winner', 'world']\n",
      "X_test:\n",
      "====\n",
      "[[0 0 0 1 0 1 0]\n",
      " [0 1 1 0 0 0 0]\n",
      " [1 0 1 0 0 0 0]\n",
      " [0 0 0 1 1 0 0]\n",
      " [0 1 0 0 0 0 1]\n",
      " [0 0 0 0 1 1 0]]\n",
      "====\n",
      "\n",
      "Spam classification (1 means document is spam):\n",
      "====\n",
      "[1. 0. 0. 1. 0. 1.]\n",
      "====\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model.logistic import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "'''\n",
    "Building a labelled dataset of documents\n",
    "'''\n",
    "doc0 = \"hello world\"\n",
    "doc1 = \"foo bar\"\n",
    "doc2 = \"lottery prize winner\"\n",
    "docs_train = [doc0] * 100 + [doc1] * 100 + [doc2] * 4 \n",
    "docs_test = [\n",
    "             \"lottery winner\",\n",
    "             \"hello foo\", \n",
    "             \"hello bar\", \n",
    "             \"lottery prize\", \n",
    "             \"world foo\",\n",
    "             \"prize winner\",\n",
    "            ]\n",
    "'''\n",
    "    Converting documents to feature vectors\n",
    "'''\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(docs_train).toarray()\n",
    "features = vectorizer.get_feature_names()\n",
    "print(f'Features: {features}')\n",
    "# print(f'X_train:\\n====\\n{X_train}\\n====\\n')\n",
    "Y_train = np.array([[0] * 100 + [0] * 100 + [1.0] * 4]).reshape(204,)\n",
    "#print(f'Y_train:\\n====\\n{Y_train}\\n====\\n')\n",
    "\n",
    "X_test = vectorizer.transform(docs_test).toarray(docs_test)\n",
    "print(f'X_test:\\n====\\n{X_test}\\n====\\n')\n",
    "\n",
    "'''\n",
    "Choosing the model\n",
    "'''\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "\n",
    "'''\n",
    "    Train the model\n",
    "'''\n",
    "#TBD\n",
    "classifier.fit(X_train, Y_train)\n",
    "\n",
    "'''\n",
    "    Test the model\n",
    "'''\n",
    "#TBD Use fitted model to predict if the documents in test data are spam or not\n",
    "predictions = classifier.predict(X_test)\n",
    "print(f'Spam classification (1 means document is spam):\\n====\\n{predictions}\\n====\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendation (Similar Documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ['bar', 'foo', 'hello', 'lottery', 'prize', 'winner', 'world']\n",
      "X_train:\n",
      "====\n",
      "[[0 0 2 0 0 0 1]\n",
      " [0 0 1 0 0 0 2]\n",
      " [0 0 2 0 0 0 2]\n",
      " [1 2 0 0 0 0 0]\n",
      " [2 1 0 0 0 0 0]\n",
      " [2 2 0 0 0 0 0]\n",
      " [0 0 0 1 1 1 0]\n",
      " [0 0 0 1 1 0 0]\n",
      " [0 0 0 3 1 1 0]]\n",
      "====\n",
      "\n",
      "X_test:\n",
      "====\n",
      "[[1 1 0 0 0 0 0]\n",
      " [0 0 1 1 1 0 0]]\n",
      "====\n",
      "\n",
      "Indices of nearest documents:\n",
      "====\n",
      "[[3 4]\n",
      " [7 6]]\n",
      "====\n",
      "\n",
      "Distances of nearest documents:\n",
      "====\n",
      "[[1.         1.        ]\n",
      " [1.         1.41421356]]\n",
      "====\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "\n",
    "'''\n",
    "Building a dataset of documents\n",
    "'''\n",
    "\n",
    "'''\n",
    "    Traing docs\n",
    "'''\n",
    "doc0 = \"hello hello world \"\n",
    "doc1 = \"hello world world\"\n",
    "doc2 = \"hello world hello world\"\n",
    "doc3 = \"foo foo bar \"\n",
    "doc4 = \"foo bar bar\"\n",
    "doc5 = \"foo bar foo bar\"\n",
    "doc6 = \"lottery prize winner\"\n",
    "doc7 = \"lottery prize\"\n",
    "doc8 = \"lottery lottery lottery prize winner\"\n",
    "docs_train = [doc0, doc1, doc2, doc3, doc4, doc5, doc6, doc7, doc8]\n",
    "\n",
    "'''\n",
    "    Test docs\n",
    "'''\n",
    "doct0 = \"foo bar\"\n",
    "doct1 = \"lottery prize hello\"\n",
    "docs_test = [doct0, doct1]\n",
    "\n",
    "\n",
    "'''\n",
    "    Converting documents to feature vectors\n",
    "'''\n",
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(docs_train).toarray()\n",
    "features = vectorizer.get_feature_names()\n",
    "print(f'Features: {features}')\n",
    "print(f'X_train:\\n====\\n{X_train}\\n====\\n')\n",
    "X_test = vectorizer.transform(docs_test).toarray()\n",
    "print(f'X_test:\\n====\\n{X_test}\\n====\\n')\n",
    "\n",
    "'''\n",
    "Choosing the model\n",
    "'''\n",
    "model = NearestNeighbors(n_neighbors=2, algorithm='brute')\n",
    "\n",
    "'''\n",
    "    Train the model\n",
    "'''\n",
    "#TBD\n",
    "model.fit(X_train)\n",
    "\n",
    "'''\n",
    "    Test the model\n",
    "'''\n",
    "#TBD\n",
    "distances, indices = model.kneighbors(X_test)\n",
    "print(f'Indices of nearest documents:\\n====\\n{indices}\\n====\\n')\n",
    "print(f'Distances of nearest documents:\\n====\\n{distances}\\n====\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly Detection (Detecting Fraud Transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arawat/anaconda3/lib/python3.6/site-packages/sklearn/covariance/robust_covariance.py:622: UserWarning: The covariance matrix associated to your dataset is not full rank\n",
      "  warnings.warn(\"The covariance matrix associated to your dataset \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraud Transaction Prediction (-1 means anomaly/outlier i.e fraud):\n",
      "====\n",
      "[-1 -1  1]\n",
      "====\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "'''\n",
    "features = [\"amount_category\", \"merchant_web_mobile\", \"dow\",  \"hour\" ]\n",
    "feature description\n",
    "    amount_category       0 if amount < £100\n",
    "                          1 if  £100 <= amount < £500)\n",
    "                          2 if  £500 <= amount\n",
    "\n",
    "    merchant_web_mobile   0 if  transaction done in shop\n",
    "                          1 if  transaction done through web\n",
    "                          2 if  transaction done through mobile    \n",
    "    \n",
    "    dow                   transaction day of week (1-7)\n",
    "    hour                  transaction hour (1-24)\n",
    "'''\n",
    "def normal_txn_hour():\n",
    "    return random.randint(9,18)\n",
    "def normal_txn_dow():\n",
    "    return random.randint(1,5)\n",
    "\n",
    "'''\n",
    "Building a hypothetical dataset of normal transactions\n",
    "'''\n",
    "N = 1000\n",
    "X_train = [[0, 0, normal_txn_dow(), normal_txn_hour()] for i in range(N)]\n",
    "# print(*X_train, sep = \"\\n\")\n",
    "\n",
    "'''\n",
    "Building a hypothetical test dataset: some fraud transactions and some normal\n",
    "'''\n",
    "X_test = [\n",
    "    [1,2,6,23],\n",
    "    [1,2,6,23],\n",
    "    [0,0,2,11],\n",
    "]\n",
    "\n",
    "'''\n",
    "Choosing a model\n",
    "'''\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "model = EllipticEnvelope()\n",
    "\n",
    "'''\n",
    "    Train the model so that it understand whats normal transaction\n",
    "'''\n",
    "\n",
    "#TBD\n",
    "model.fit(X_train)\n",
    "\n",
    "'''\n",
    "   Use the trained model to predict if the given transactions are fraudlent or not\n",
    "'''\n",
    "\n",
    "#TBD \n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "\n",
    "print(f'Fraud Transaction Prediction (-1 means anomaly/outlier i.e fraud):\\n====\\n{predictions}\\n====\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Analytics (Important nodes in the network)\n",
    "qualitative treatment\n",
    " - truths  (Select <attributes> from where <filter> )\n",
    " - deeper truths (Select <agg> from where <filter> group by)\n",
    " - revelations (page_rank(G(n)) )\n",
    "concept of centrality\n",
    "    \n",
    "### Expressing problems as Graph\n",
    "- Social Network, transport, communication, infrasturucture, documents\n",
    "- Links\n",
    "- Software 2.0 approach (express problem as a graph & apply graph algorithms)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
