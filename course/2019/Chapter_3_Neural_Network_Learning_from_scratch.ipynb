{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Rectified Linear Unit (relu Function) x if x > 0 else x\n",
    "\n",
    "    :param Z: input tensor\n",
    "    :return:  Relu output,  and input stored in cache\n",
    "    \"\"\"\n",
    "    A = np.maximum(0, Z)\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Sigmoid Function\n",
    "\n",
    "    :param Z: input tensor\n",
    "    :return: Sigmoid output and input stored in cache\n",
    "    \"\"\"\n",
    "\n",
    "    A = 1 / (1 + np.exp(-Z))\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Derivative of relu function\n",
    "\n",
    "    :param dA:\n",
    "    :param cache:\n",
    " \n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True)  # just converting dz to a correct object.\n",
    "\n",
    "    # When z <= 0, you should set dz to 0 as well.\n",
    "    dZ[Z <= 0] = 0\n",
    "\n",
    "    assert (dZ.shape == Z.shape)\n",
    "\n",
    "    return dZ\n",
    "\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Derivative of sigmoid function\n",
    "\n",
    "    :param dA:\n",
    "    :param cache:\n",
    "    \n",
    "    :return:\n",
    "    \"\"\"\n",
    "    Z = cache\n",
    "\n",
    "    s = 1 / (1 + np.exp(-Z))\n",
    "    dZ = dA * s * (1 - s)\n",
    "\n",
    "    assert (dZ.shape == Z.shape)\n",
    "    return dZ\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(n_x, n_h1, n_h2, n_y):\n",
    "    \"\"\"\n",
    "\n",
    "    Initialize weights tensor\n",
    "\n",
    "    :param n_x: size of the input layer\n",
    "    :param n_h1: size of the hidden layer 1\n",
    "    :param n_h2: size of the hidden layer 2\n",
    "    :param n_y: size of the output layer\n",
    "\n",
    "    :return:     weights -- python dictionary containing initialized weights:\n",
    "                    W1 -- weight matrix of shape (n_h1, n_x)\n",
    "                    b1 -- bias vector of shape (n_h1, 1)\n",
    "                    W2 -- weight matrix of shape (n_h2, n_h1)\n",
    "                    b2 -- bias vector of shape (n_h2, 1)\n",
    "                    W3 -- weight matrix of shape (n_y, n_h2)\n",
    "                    b3 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "\n",
    "    W1 = np.random.randn(n_h1, n_x) * 0.01\n",
    "    b1 = np.zeros((n_h1, 1))\n",
    "    W2 = np.random.randn(n_h2, n_h1) * 0.01\n",
    "    b2 = np.zeros((n_h2, 1))\n",
    "    W3 = np.random.randn(n_y, n_h2) * 0.01\n",
    "    b3 = np.zeros((n_y, 1))\n",
    "\n",
    "    assert (W1.shape == (n_h1, n_x))\n",
    "    assert (b1.shape == (n_h1, 1))\n",
    "    assert (W2.shape == (n_h2, n_h1))\n",
    "    assert (b2.shape == (n_h2, 1))\n",
    "    assert (W3.shape == (n_y, n_h2))\n",
    "    assert (b3.shape == (n_y, 1))\n",
    "\n",
    "    weights = {\n",
    "            \"W1\": W1,\n",
    "            \"b1\": b1,\n",
    "            \"W2\": W2,\n",
    "            \"b2\": b2,\n",
    "            \"W3\": W3,\n",
    "            \"b3\": b3\n",
    "    }\n",
    "\n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_forward(A, W, b, activation_function):\n",
    "    \"\"\"\n",
    "    Step forward one layer in a deep neural network\n",
    "\n",
    "    :param A: Input tensor for the layer\n",
    "    :param W: Weight tensor for the layer\n",
    "    :param b: Bias tensor for the layer\n",
    "    :param activation_function: Activation function to be applied to the layer\n",
    "\n",
    "    :return: Output Activation tensor and inputs stored in cache\n",
    "    \"\"\"\n",
    "\n",
    "    Z = np.matmul(W, A) + b\n",
    "\n",
    "    assert (Z.shape == (W.shape[0], A.shape[1]))\n",
    "    linear_cache = (A, W, b)\n",
    "\n",
    "    A_next, activation_cache = activation_function(Z)\n",
    "\n",
    "    assert (A_next.shape == (W.shape[0], A_next.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A_next, cache\n",
    "\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "\n",
    "    Cross Entropy Error (Cost) between Y-Hat and Y  (Y-Hat is activation of Lth Layer hence AL)\n",
    "\n",
    "    :param AL: Activation of Lth Layer i.e Y-Hat\n",
    "    :param Y: Actual Labels (Ground Truth)\n",
    "\n",
    "    :return: Cost (How close are we?)\n",
    "\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    cost = -(np.sum(Y * np.log(AL) + (1 - Y) * np.log(1 - AL))) / m\n",
    "    cost = np.squeeze(cost)\n",
    "    assert (cost.shape == ())\n",
    "\n",
    "    return cost\n",
    "\n",
    "\n",
    "def step_backward(dA, cache, activation_function_backward):\n",
    "    \"\"\"\n",
    "    Step back one layer in deep neural network,\n",
    "    i.e Find out how much adjustment is needed in weights in this layer\n",
    "\n",
    "    :param dA:\n",
    "    :param cache:\n",
    "    :param activation_function_backward:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    linear_cache, activation_cache = cache\n",
    "\n",
    "    dZ = activation_function_backward(dA, activation_cache)\n",
    "\n",
    "    A_prev, W, b = linear_cache\n",
    "    N = A_prev.shape[1]\n",
    "\n",
    "    dW = np.matmul(dZ, A_prev.transpose()) / N\n",
    "    db = np.sum(dZ, axis=1, keepdims=True) / N\n",
    "    dA_prev = np.matmul(W.transpose(), dZ)\n",
    "\n",
    "\n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def update_weights(weights, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update weights across all layers\n",
    "    After our back-propogation we know how much weight & bias adjustment is needed\n",
    "    in each layer, This function performs that update.\n",
    "\n",
    "    :param weights: Original weights\n",
    "    :param grads: Changes that needs to be made to weights\n",
    "    :param learning_rate: learning rate of algorithm. (a step length in gradient descent)\n",
    "\n",
    "    :return: updated weigths\n",
    "    \"\"\"\n",
    "    L = len(weights)//2    # number of layers in the neural network\n",
    "\n",
    "    for l in range(L):\n",
    "        weights[\"W\" + str(l + 1)] = weights[\"W\" + str(l + 1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n",
    "        weights[\"b\" + str(l + 1)] = weights[\"b\" + str(l + 1)] - learning_rate * grads[\"db\" + str(l + 1)]\n",
    "\n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_3_layer_model(X, Y, layers_dims, learning_rate=0.0075, num_iterations=10000, print_cost=False):\n",
    "\n",
    "    np.random.seed(1)\n",
    "    grads = {}\n",
    "    costs = []  # to keep track of the cost\n",
    "    m = X.shape[1]  # number of examples\n",
    "    \n",
    "    (n_x, n_h1, n_h2, n_y) = layers_dims\n",
    "\n",
    "    weights = init_weights(n_x, n_h1, n_h2, n_y)\n",
    "\n",
    "    # gradient descent\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: (LINEAR -> RELU) * 2 -> LINEAR -> SIGMOID.\n",
    "\n",
    "        # Retrieve W1, b1, W2, b2 from weights\n",
    "        W1 = weights[\"W1\"]\n",
    "        b1 = weights[\"b1\"]\n",
    "        W2 = weights[\"W2\"]\n",
    "        b2 = weights[\"b2\"]\n",
    "        W3 = weights[\"W3\"]\n",
    "        b3 = weights[\"b3\"]\n",
    "\n",
    "\n",
    "        A1, cache1 = step_forward(X, W1, b1, relu)\n",
    "        A2, cache2 = step_forward(A1, W2, b2, relu)\n",
    "        A3, cache3 = step_forward(A2, W3, b3, sigmoid)\n",
    "\n",
    "        cost = compute_cost(A3, Y)\n",
    "\n",
    "        # Initializing backward propagation\n",
    "        dA3 = - (np.divide(Y, A3) - np.divide(1 - Y, 1 - A3))\n",
    "\n",
    "        # Backward propagation.\n",
    "        dA2, dW3, db3 = step_backward(dA3, cache3, sigmoid_backward)\n",
    "        dA1, dW2, db2 = step_backward(dA2, cache2, relu_backward)\n",
    "        dA0, dW1, db1 = step_backward(dA1, cache1, relu_backward)\n",
    "\n",
    "        grads['dW1'] = dW1\n",
    "        grads['db1'] = db1\n",
    "        grads['dW2'] = dW2\n",
    "        grads['db2'] = db2\n",
    "        grads['dW3'] = dW3\n",
    "        grads['db3'] = db3\n",
    "\n",
    "\n",
    "        # Update weights.\n",
    "        weights = update_weights(weights, grads, learning_rate)\n",
    "\n",
    "\n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "\n",
    "    # plot the cost\n",
    "\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "\n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of data = 1797\n",
      "1797\n",
      "dict_keys(['data', 'target', 'target_names', 'images', 'DESCR'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "digits = datasets.load_digits()\n",
    "images_and_labels = list(zip(digits.images, digits.target))\n",
    "print(f'len of data = {len(digits.data)}')\n",
    "print(len(images_and_labels))\n",
    "plt.figure(figsize=(8,6))\n",
    "for index, (image, label) in enumerate(images_and_labels[1:6]):\n",
    "    plt.subplot(1, 6, index + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title('Training: %i' % label)\n",
    "\n",
    "print(digits.keys())\n",
    "# print(digits['DESCR'])\n",
    "# print(len(digits['data'][0]))\n",
    "# print(len(digits['data']))\n",
    "# print(digits['data'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the digits dataset\n",
    "N = len(digits.data)\n",
    "X = digits.data\n",
    "encoder = preprocessing.OneHotEncoder()\n",
    "target_temp = digits.target.reshape(-1,1)\n",
    "encoder.fit(target_temp)\n",
    "Y = encoder.transform(target_temp).toarray()\n",
    "\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=42)\n",
    "\n",
    "X_train = np.transpose(X_train)\n",
    "X_test = np.transpose(X_test)\n",
    "Y_train = np.transpose(Y_train)\n",
    "Y_test = np.transpose(Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 6.9337754140142325\n",
      "Cost after iteration 100: 3.271578220322025\n",
      "Cost after iteration 200: 3.270022413903926\n",
      "Cost after iteration 300: 3.268442260472516\n",
      "Cost after iteration 400: 3.2663557672959245\n",
      "Cost after iteration 500: 3.262618494692561\n",
      "Cost after iteration 600: 3.253073909194501\n",
      "Cost after iteration 700: 3.213067053157645\n",
      "Cost after iteration 800: 2.9276733467399394\n",
      "Cost after iteration 900: 2.4317231059329805\n",
      "Cost after iteration 1000: 2.0726213697042954\n",
      "Cost after iteration 1100: 1.9189583478394585\n",
      "Cost after iteration 1200: 1.7711967482362891\n",
      "Cost after iteration 1300: 1.579725180166698\n",
      "Cost after iteration 1400: 1.2846667673705825\n",
      "Cost after iteration 1500: 1.0648440070615577\n",
      "Cost after iteration 1600: 0.9285833654590022\n",
      "Cost after iteration 1700: 0.8290576218131815\n",
      "Cost after iteration 1800: 0.7482595022039127\n",
      "Cost after iteration 1900: 0.6776640318118191\n",
      "Cost after iteration 2000: 0.6158503297715262\n",
      "Cost after iteration 2100: 0.5614767850562544\n",
      "Cost after iteration 2200: 0.5135494636413027\n",
      "Cost after iteration 2300: 0.46957923564011694\n",
      "Cost after iteration 2400: 0.4290451309396304\n",
      "Cost after iteration 2500: 0.39255875160760934\n",
      "Cost after iteration 2600: 0.3588481221253051\n",
      "Cost after iteration 2700: 0.32878989162025135\n",
      "Cost after iteration 2800: 0.30241835146879353\n",
      "Cost after iteration 2900: 0.2790962172343895\n",
      "Cost after iteration 3000: 0.25783935197841734\n",
      "Cost after iteration 3100: 0.23871899094599341\n",
      "Cost after iteration 3200: 0.2215311244058027\n",
      "Cost after iteration 3300: 0.20594288074963352\n",
      "Cost after iteration 3400: 0.19186107977473624\n",
      "Cost after iteration 3500: 0.17918820399072846\n",
      "Cost after iteration 3600: 0.16748811470098957\n",
      "Cost after iteration 3700: 0.15671129551668797\n",
      "Cost after iteration 3800: 0.14691898240228288\n",
      "Cost after iteration 3900: 0.1378054365474891\n",
      "Cost after iteration 4000: 0.12953921873332747\n",
      "Cost after iteration 4100: 0.12192782574932663\n",
      "Cost after iteration 4200: 0.11469074113606421\n",
      "Cost after iteration 4300: 0.10803153203005735\n",
      "Cost after iteration 4400: 0.10188043815952405\n",
      "Cost after iteration 4500: 0.09613792384150646\n",
      "Cost after iteration 4600: 0.09079019724794224\n",
      "Cost after iteration 4700: 0.08584782123386946\n",
      "Cost after iteration 4800: 0.081263107082959\n",
      "Cost after iteration 4900: 0.07702363169994413\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_x = 64\n",
    "n_h1 = 40\n",
    "n_h2 = 30\n",
    "n_y = 10\n",
    "layers_dims = (n_x, n_h1, n_h2, n_y)\n",
    "\n",
    "weights = nn_3_layer_model(X_train, Y_train, layers_dims = (n_x, n_h1, n_h2, n_y), num_iterations = 5000, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, weights):\n",
    "    \"\"\"\n",
    "    Given weights of deep neural network, predict the output for X\n",
    "\n",
    "    :param X: input tensor\n",
    "    :param weights: Weights and Bias of the network\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    W1 = weights[\"W1\"]\n",
    "    b1 = weights[\"b1\"]\n",
    "    W2 = weights[\"W2\"]\n",
    "    b2 = weights[\"b2\"]\n",
    "    W3 = weights[\"W3\"]\n",
    "    b3 = weights[\"b3\"]\n",
    "\n",
    "    A1, cache1 = step_forward(X, W1, b1, relu)\n",
    "    A2, cache2 = step_forward(A1, W2, b2, relu)\n",
    "    A3, cache3 = step_forward(A2, W3, b3, sigmoid)\n",
    "    A3 = np.around(A3)\n",
    "    return A3\n",
    "\n",
    "def compute_accuracy(X, y, weights):\n",
    "    \"\"\"\n",
    "    Compute predictions for X given the weights and then find out how accurate are the predictions\n",
    "    by comparing with y\n",
    "\n",
    "    :param X: Input Tensors to be predicted\n",
    "    :param y: Ground Truth Labels\n",
    "    :param weights: Weight (Model)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    N = X.shape[1]\n",
    "    A3 = predict(X, weights)\n",
    "    res = A3 == y\n",
    "    accuracy = np.sum(res.all(axis=0)) / N\n",
    "    print(\"Accuracy: \" + str(accuracy))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9850374064837906\n",
      "Accuracy: 0.9124579124579124\n"
     ]
    }
   ],
   "source": [
    "accuracy_train = compute_accuracy(X_train, Y_train, weights)\n",
    "accuracy_test = compute_accuracy(X_test, Y_test, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_test[:,[1]]\n",
    "ax_test = X_test[:, [19, 55, 300]]\n",
    "ay_test = Y_test[:, [19, 55, 300]]\n",
    "print(ay_test)\n",
    "\n",
    "predict(ax_test, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
