{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "def square(x):\n",
    "    #TBD\n",
    "    pass\n",
    "\n",
    "def times2(x):\n",
    "    #TBD\n",
    "    pass\n",
    "\n",
    "def average(x1, x2, x3, x4, x5):\n",
    "    #TBD\n",
    "    pass\n",
    "\n",
    "print(times2(10) == 20)\n",
    "print(square(6) == 36)\n",
    "print(average(1,2,3,4,5) == 3.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reverse engineering functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "'''\n",
    "+--------+--+--------+--+--------+--+--------------+\n",
    "| f1     |  | f2     |  | f3     |  | f4           |\n",
    "+--------+  +--------+  +--------+  +--------------+\n",
    "| x | y  |  | x | y  |  | x  | y |  | x1 | x2 | y  |\n",
    "+---+----+  +---+----+  +----+---+  +----+----+----+\n",
    "| 3 | 6  |  | 3 | 6  |  | 1  | 0 |  | 2  | 3  | 4  |\n",
    "+---+----+  +---+----+  +----+---+  +----+----+----+\n",
    "| 7 | 14 |  | 7 | 28 |  | 7  | 1 |  | 8  | 5  | 9  |\n",
    "+---+----+  +---+----+  +----+---+  +----+----+----+\n",
    "| 8 | 16 |  | 4 | 10 |  | 3  | 1 |  | 6  | 4  | 7  |\n",
    "+---+----+  +---+----+  +----+---+  +----+----+----+\n",
    "| 1 | 2  |  | 1 | 1  |  | 2  | 1 |  | 10 | 2  | 7  |\n",
    "+---+----+  +---+----+  +----+---+  +----+----+----+\n",
    "| 5 | 10 |  | 9 | 45 |  | 9  | 0 |  | 4  | 2  | 4  |\n",
    "+---+----+  +---+----+  +----+---+  +----+----+----+\n",
    "| 9 | 18 |  | 6 | 21 |  | 12 | 0 |  | 16 | 7  | 15 |\n",
    "+---+----+--+---+----+--+----+---+--+----+----+----+\n",
    "'''\n",
    "def f1(x):\n",
    "    #TBD\n",
    "    pass\n",
    "\n",
    "def f2(x):\n",
    "    #TBD\n",
    "    pass\n",
    "\n",
    "\n",
    "def f4(x1, x2):\n",
    "    #TBD\n",
    "    pass\n",
    "\n",
    "\n",
    "print(f1(11) == 22)\n",
    "print(f2(5) == 15.0)\n",
    "print(f4(10, 7) == 12.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Software 2.0?\n",
    "[Andrej Karpathy](https://medium.com/@karpathy/software-2-0-a64152b37c35)\n",
    "\n",
    " - 1.0\n",
    "     - Function\n",
    "     - Specifications, Code, small {X,Y} which is used to test specs\n",
    "     - Code\n",
    "     - Concrete Answers\n",
    " - 2.0 \n",
    "     - ???\n",
    "     - Large {X,Y} which is used to learn/test a model\n",
    "     - ???\n",
    "     - Soft Answers\n",
    "\n",
    "### Machine Learning and Software 2.0\n",
    "\n",
    "### Types of ML\n",
    "    1. Supervised Learning\n",
    "        - Regression\n",
    "        - Classification\n",
    "    2. Unsupervised Learning\n",
    "    3. Reinforcement Learning\n",
    "\n",
    "### When to apply ML?\n",
    "    1. ???\n",
    "    2. ???\n",
    "    3. ???\n",
    "\n",
    "\n",
    "### Features, Signals, Attributes, *x*\n",
    " - Text as features\n",
    " - Videos, Pictures as features\n",
    " - Time as features\n",
    " \n",
    "### Label, Targets, *y*\n",
    "\n",
    "### *x* and *y* \n",
    "\n",
    "### *X* and *Y*\n",
    "\n",
    "### *n* and *N*\n",
    "\n",
    "### Dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This Class:\n",
    "\n",
    "### What Level?\n",
    " - Input: Numbers.\n",
    " - Models: Some canned, Some from scratch\n",
    " - Evaluation: Some canned, Some from scratch\n",
    " \n",
    "### Deep Dives\n",
    " - ML Evaluation Metrics\n",
    " - Linear Regression\n",
    " - Neural Network (Back Propogation Algorithm)\n",
    " - Collaborative Filtering\n",
    " - Graph Centrality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression (Predicting House Prices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted values: []\n",
      "Actual values: [16.8 22.4 20.6 23.9 22.  11.9]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Load Boston, house price dataset\n",
    "'''\n",
    "from sklearn import datasets\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "boston = datasets.load_boston()\n",
    "\n",
    "# print(f'Boston dataset info: {boston.keys()}')\n",
    "# print(f'Dataset Description ==> {boston[\"DESCR\"]}\\n==========\\n')\n",
    "# print(f'X ==> boston[\"data\"]')\n",
    "# print(f'Y ==> boston[\"target\"]')\n",
    "\n",
    "# print(f'features ==> {boston[\"feature_names\"]}')\n",
    "# print(f'n ==> {len(boston[\"feature_names\"])}')\n",
    "# print(f'N ==> {len(boston[\"data\"])}')\n",
    "# print(f'Shape of X ==> {boston[\"data\"].shape}')\n",
    "# print(f'Shape of Y ==> {boston[\"target\"].shape}')\n",
    "# print(f\"First data point: x, X[0]: {boston.data[0]}\")\n",
    "# print(f\"First label: y, Y[0]: {boston.target[0]}\")\n",
    "\n",
    "\n",
    "N = 500\n",
    "X_train = boston.data[:N]\n",
    "Y_train = boston.target[:N]\n",
    "X_test = boston.data[500:]\n",
    "Y_test = boston.target[500:]\n",
    "\n",
    "model = linear_model.LinearRegression()\n",
    "\n",
    "#TBD 1: Fit the model to training data\n",
    "\n",
    "#TBD 2: Use fitted model to predict the house prices of houses in test data \n",
    "predictions = []\n",
    "\n",
    "print(f\"Predicted values: {predictions}\")\n",
    "print(f\"Actual values: {Y_test}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification (Detecting Spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ['bar', 'foo', 'hello', 'lottery', 'prize', 'winner', 'world']\n",
      "X_test:\n",
      "====\n",
      "[[0 0 0 1 0 1 0]\n",
      " [0 1 1 0 0 0 0]\n",
      " [1 0 1 0 0 0 0]\n",
      " [0 0 0 1 1 0 0]\n",
      " [0 1 0 0 0 0 1]\n",
      " [0 0 0 0 1 1 0]]\n",
      "====\n",
      "\n",
      "Spam classification (1 means document is spam):\n",
      "====\n",
      "[]\n",
      "====\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model.logistic import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "'''\n",
    "Building a labelled dataset of documents\n",
    "'''\n",
    "doc0 = \"hello world\"\n",
    "doc1 = \"foo bar\"\n",
    "doc2 = \"lottery prize winner\"\n",
    "docs_train = [doc0] * 100 + [doc1] * 100 + [doc2] * 4 \n",
    "docs_test = [\n",
    "             \"lottery winner\",\n",
    "             \"hello foo\", \n",
    "             \"hello bar\", \n",
    "             \"lottery prize\", \n",
    "             \"world foo\",\n",
    "             \"prize winner\",\n",
    "            ]\n",
    "'''\n",
    "    Converting documents to feature vectors\n",
    "'''\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(docs_train).toarray()\n",
    "features = vectorizer.get_feature_names()\n",
    "print(f'Features: {features}')\n",
    "# print(f'X_train:\\n====\\n{X_train}\\n====\\n')\n",
    "Y_train = np.array([[0] * 100 + [0] * 100 + [1.0] * 4]).reshape(204,)\n",
    "#print(f'Y_train:\\n====\\n{Y_train}\\n====\\n')\n",
    "\n",
    "X_test = vectorizer.transform(docs_test).toarray(docs_test)\n",
    "print(f'X_test:\\n====\\n{X_test}\\n====\\n')\n",
    "\n",
    "'''\n",
    "Choosing the model\n",
    "'''\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "\n",
    "'''\n",
    "    Train the model\n",
    "'''\n",
    "#TBD Fit the model to training data\n",
    "\n",
    "'''\n",
    "    Test the model\n",
    "'''\n",
    "#TBD Use fitted model to predict if the documents in test data are spam or not\n",
    "predictions = []\n",
    "print(f'Spam classification (1 means document is spam):\\n====\\n{predictions}\\n====\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendation (Similar Documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ['bar', 'foo', 'hello', 'lottery', 'prize', 'winner', 'world']\n",
      "X_train:\n",
      "====\n",
      "[[0 0 2 0 0 0 1]\n",
      " [0 0 1 0 0 0 2]\n",
      " [0 0 2 0 0 0 2]\n",
      " [1 2 0 0 0 0 0]\n",
      " [2 1 0 0 0 0 0]\n",
      " [2 2 0 0 0 0 0]\n",
      " [0 0 0 1 1 1 0]\n",
      " [0 0 0 1 1 0 0]\n",
      " [0 0 0 3 1 1 0]]\n",
      "====\n",
      "\n",
      "X_test:\n",
      "====\n",
      "[[1 1 0 0 0 0 0]\n",
      " [0 0 1 1 1 0 0]]\n",
      "====\n",
      "\n",
      "Indices of nearest documents:\n",
      "====\n",
      "0\n",
      "====\n",
      "\n",
      "Distances of nearest documents:\n",
      "====\n",
      "0\n",
      "====\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "\n",
    "'''\n",
    "Building a dataset of documents\n",
    "'''\n",
    "\n",
    "'''\n",
    "    Traing docs\n",
    "'''\n",
    "doc0 = \"hello hello world \"\n",
    "doc1 = \"hello world world\"\n",
    "doc2 = \"hello world hello world\"\n",
    "doc3 = \"foo foo bar \"\n",
    "doc4 = \"foo bar bar\"\n",
    "doc5 = \"foo bar foo bar\"\n",
    "doc6 = \"lottery prize winner\"\n",
    "doc7 = \"lottery prize\"\n",
    "doc8 = \"lottery lottery lottery prize winner\"\n",
    "docs_train = [doc0, doc1, doc2, doc3, doc4, doc5, doc6, doc7, doc8]\n",
    "\n",
    "'''\n",
    "    Test docs\n",
    "'''\n",
    "doct0 = \"foo bar\"\n",
    "doct1 = \"lottery prize hello\"\n",
    "docs_test = [doct0, doct1]\n",
    "\n",
    "\n",
    "'''\n",
    "    Converting documents to feature vectors\n",
    "'''\n",
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(docs_train).toarray()\n",
    "features = vectorizer.get_feature_names()\n",
    "print(f'Features: {features}')\n",
    "print(f'X_train:\\n====\\n{X_train}\\n====\\n')\n",
    "X_test = vectorizer.transform(docs_test).toarray()\n",
    "print(f'X_test:\\n====\\n{X_test}\\n====\\n')\n",
    "\n",
    "'''\n",
    "Choosing the model\n",
    "'''\n",
    "model = NearestNeighbors(n_neighbors=3, algorithm='brute')\n",
    "\n",
    "'''\n",
    "    Train the model\n",
    "'''\n",
    "#TBD\n",
    "\n",
    "'''\n",
    "    Test the model, by getting  k=3 most similar documents, and their distances to the ones in test docs \n",
    "'''\n",
    "#TBD\n",
    "indices, distances = (0,0)\n",
    "\n",
    "print(f'Indices of nearest documents:\\n====\\n{indices}\\n====\\n')\n",
    "print(f'Distances of nearest documents:\\n====\\n{distances}\\n====\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly Detection (Detecting Fraud Transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraud Transaction Prediction (-1 means anomaly/outlier i.e fraud):\n",
      "====\n",
      "[]\n",
      "====\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "'''\n",
    "features = [\"amount_category\", \"merchant_web_mobile\", \"dow\",  \"hour\" ]\n",
    "feature description\n",
    "    amount_category       0 if amount < £100\n",
    "                          1 if  £100 <= amount < £500)\n",
    "                          2 if  £500 <= amount\n",
    "\n",
    "    merchant_web_mobile   0 if  transaction done in shop\n",
    "                          1 if  transaction done through web\n",
    "                          2 if  transaction done through mobile    \n",
    "    \n",
    "    dow                   transaction day of week (1-7)\n",
    "    hour                  transaction hour (1-24)\n",
    "'''\n",
    "def normal_txn_hour():\n",
    "    return random.randint(9,18)\n",
    "def normal_txn_dow():\n",
    "    return random.randint(1,5)\n",
    "\n",
    "'''\n",
    "Building a hypothetical dataset of normal transactions\n",
    "'''\n",
    "N = 1000\n",
    "X_train = [[0, 0, normal_txn_dow(), normal_txn_hour()] for i in range(N)]\n",
    "# print(*X_train, sep = \"\\n\")\n",
    "\n",
    "'''\n",
    "Building a hypothetical test dataset: some fraud transactions and some normal\n",
    "'''\n",
    "X_test = [\n",
    "    [1,2,6,23],\n",
    "    [1,2,6,23],\n",
    "    [0,0,2,11],\n",
    "]\n",
    "\n",
    "'''\n",
    "Choosing a model\n",
    "'''\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "model = EllipticEnvelope()\n",
    "\n",
    "'''\n",
    "    Train the model so that it understand whats normal transaction\n",
    "'''\n",
    "\n",
    "#TBD\n",
    "\n",
    "\n",
    "'''\n",
    "   Use the trained model to predict if the given transactions are fraudlent or not\n",
    "'''\n",
    "\n",
    "#TBD \n",
    "predictions = []\n",
    "\n",
    "\n",
    "print(f'Fraud Transaction Prediction (-1 means anomaly/outlier i.e fraud):\\n====\\n{predictions}\\n====\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Analytics (Important nodes in the network)\n",
    "qualitative treatment\n",
    " - truths  (Select <attributes> from where <filter> )\n",
    " - deeper truths (Select <agg> from where <filter> group by)\n",
    " - revelations (page_rank(G(n)) )\n",
    "concept of centrality\n",
    "    \n",
    "### Expressing problems as Graph\n",
    "- Social Network, transport, communication, infrasturucture, documents\n",
    "- Links\n",
    "- Software 2.0 approach (express problem as a graph & apply graph algorithms)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
